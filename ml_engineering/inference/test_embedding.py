import json
from sentence_transformers import SentenceTransformer
import numpy as np

def run_simple_inference():
    # 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì½ê¸°
    with open("data/processed_gumi_policy.json", "r", encoding="utf-8") as f:
        policy = json.load(f)

    # 2. í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì²˜ìŒ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œë¡œ ì¸í•´ ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)
    # ìš°ë¦¬ requirements.txtì— sentence-transformersê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    model = SentenceTransformer('jhgan/ko-sbert-sts')

    # 3. í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (Vectorization)
    # ì œëª©ê³¼ í˜œíƒ ìš”ì•½ì„ í•©ì³ì„œ ëª¨ë¸ì— ë„£ìŠµë‹ˆë‹¤.
    text_to_embed = f"{policy['title']} {policy['eligibility']['benefit_summary']}"
    embedding = model.encode(text_to_embed)

    # 4. ê²°ê³¼ í™•ì¸ (ë§›ë³´ê¸°)
    print(f"âœ… ëª¨ë¸ ì¶”ë¡  ì™„ë£Œ!")
    print(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: {text_to_embed}")
    print(f"ğŸ“ ìƒì„±ëœ ë²¡í„° ì°¨ì›: {embedding.shape}") # ë³´í†µ 768ì°¨ì›ì˜ ìˆ«ìê°€ ë‚˜ì˜µë‹ˆë‹¤.
    print(f"ğŸ“ ë²¡í„° ìƒ˜í”Œ(ì• 5ê°œ): {embedding[:5]}")

    # 5. ê°„ë‹¨í•œ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ í…ŒìŠ¤íŠ¸
    categories = ["ì£¼ê±° ë° ì›”ì„¸ ì§€ì›", "ì¼ìë¦¬ ë° ì·¨ì—… ì§€ì›", "ì°½ì—… ë° ì‚¬ì—…ê°€ ì§€ì›", "ì¥í•™ê¸ˆ ë° í•™ìê¸ˆ"]
    category_embeddings = model.encode(categories)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì¥ ë¹„ìŠ·í•œ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°)
    scores = np.dot(category_embeddings, embedding) / (np.linalg.norm(category_embeddings, axis=1) * np.linalg.norm(embedding))
    best_idx = np.argmax(scores)
    
    print(f"ğŸ¯ ëª¨ë¸ì˜ íŒë‹¨: ì´ ì •ì±…ì€ '{categories[best_idx]}' ì¹´í…Œê³ ë¦¬ì— ê°€ì¥ ê°€ê¹ìŠµë‹ˆë‹¤! (ìœ ì‚¬ë„: {scores[best_idx]:.2f})")

if __name__ == "__main__":
    run_simple_inference()